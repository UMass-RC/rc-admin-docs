{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"identity/","text":"Centralized Identity Management Every HPC cluster needs a centralized form of user management. This is because users will be using a number of nodes across the cluster, and all UIDs/GIDs need to be kept consistent on every system. Slurm in particular cares a lot about this. Unity uses LDAP to accomplish this. There are other alternatives, but LDAP seems to be the easiest and most mature solution.","title":"Introduction"},{"location":"identity/#centralized-identity-management","text":"Every HPC cluster needs a centralized form of user management. This is because users will be using a number of nodes across the cluster, and all UIDs/GIDs need to be kept consistent on every system. Slurm in particular cares a lot about this. Unity uses LDAP to accomplish this. There are other alternatives, but LDAP seems to be the easiest and most mature solution.","title":"Centralized Identity Management"},{"location":"identity/client/","text":"Our objective is to get our clients (all the nodes on the cluster), to talk to a centralized LDAP server to obtain all user information, so that we don't have to create local users on each node. To accomplish this, we will use NSLCD, which will contact the LDAP server, and NSCD, which will cache that information. Installing NSLCD/NSCD Install from Ubuntu package repositories: apt install libpam-ldapd During the installation, you will be asked several questions: LDAP URI is the location of the ldap server. In our case, thats ldap://identity/ DN (distinguished name) is the base DN, which is usually the domain component of the LDAP server. For Unity, dc=unity,dc=rc,dc=umass,dc=edu Now it will ask where to use LDAP info. You should check passwd , group , and shadow . At this point the system will be able to begin using the centralized users. However, the ldapsearch command won't work, because its config file is seperate. In /etc/ldap/ldap.conf : BASE dc=unity,dc=rc,dc=umass,dc=edu URI ldap://identity/","title":"OpenLDAP Client"},{"location":"identity/client/#installing-nslcdnscd","text":"Install from Ubuntu package repositories: apt install libpam-ldapd During the installation, you will be asked several questions: LDAP URI is the location of the ldap server. In our case, thats ldap://identity/ DN (distinguished name) is the base DN, which is usually the domain component of the LDAP server. For Unity, dc=unity,dc=rc,dc=umass,dc=edu Now it will ask where to use LDAP info. You should check passwd , group , and shadow . At this point the system will be able to begin using the centralized users. However, the ldapsearch command won't work, because its config file is seperate. In /etc/ldap/ldap.conf : BASE dc=unity,dc=rc,dc=umass,dc=edu URI ldap://identity/","title":"Installing NSLCD/NSCD"},{"location":"identity/server/","text":"Installing SLAPD Install slapd from the Ubuntu package repository apt install slapd During the installation, you will be asked several questions: Specify the admin password for the database Specify DIT, which is your domain organized into \"domain components\", or \"dc\": dc=unity,dc=rc,dc=umass,dc=edu would correspond to unity.rc.umass.edu . This is done by convention, although you can set this to anything arbitrary. Organization name is Users . Use MDB for database. Select yes for \"move old database\" Configuring SLAPD For Unity, we need to activate a number of schemas , which are files that you can important into the ldap database that add attributes / classes for ldap objects. The included schemas are located in /etc/ldap/schema . To activate a schema, use: ldapadd -Y EXTERNAL -H ldapi:/// -f <LOCATION OF SCHEMA> On Unity, we activate: cosine.ldif nis.ldif inetorgperson.ldif ssh.ldif this ldif is not included, but ldif files are plain text so you can create it: dn: cn=openssh-lpk,cn=schema,cn=config objectClass: olcSchemaConfig cn: openssh-lpk olcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' DESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) olcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY DESC 'MANDATORY: OpenSSH LPK objectclass' MAY ( sshPublicKey $ uid ) )","title":"OpenLDAP Server"},{"location":"identity/server/#installing-slapd","text":"Install slapd from the Ubuntu package repository apt install slapd During the installation, you will be asked several questions: Specify the admin password for the database Specify DIT, which is your domain organized into \"domain components\", or \"dc\": dc=unity,dc=rc,dc=umass,dc=edu would correspond to unity.rc.umass.edu . This is done by convention, although you can set this to anything arbitrary. Organization name is Users . Use MDB for database. Select yes for \"move old database\"","title":"Installing SLAPD"},{"location":"identity/server/#configuring-slapd","text":"For Unity, we need to activate a number of schemas , which are files that you can important into the ldap database that add attributes / classes for ldap objects. The included schemas are located in /etc/ldap/schema . To activate a schema, use: ldapadd -Y EXTERNAL -H ldapi:/// -f <LOCATION OF SCHEMA> On Unity, we activate: cosine.ldif nis.ldif inetorgperson.ldif ssh.ldif this ldif is not included, but ldif files are plain text so you can create it: dn: cn=openssh-lpk,cn=schema,cn=config objectClass: olcSchemaConfig cn: openssh-lpk olcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' DESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) olcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY DESC 'MANDATORY: OpenSSH LPK objectclass' MAY ( sshPublicKey $ uid ) )","title":"Configuring SLAPD"},{"location":"main/","text":"Unity Cluster Admin Documentation This set of docs assumes a solid foundation of linux, and HPC concepts. The docs are written sequentially, so every so often there will be a certain \"milestone\" in the cluster setup.","title":"Home"},{"location":"main/#unity-cluster-admin-documentation","text":"This set of docs assumes a solid foundation of linux, and HPC concepts. The docs are written sequentially, so every so often there will be a certain \"milestone\" in the cluster setup.","title":"Unity Cluster Admin Documentation"},{"location":"shutdown/shutdown/","text":"Procedures for MGHPCC Shutdown Unity Cluster Power Down Block access from the Palo firewall rule. Shutdown all Compute Nodes Shutdown the containers/VMs running on the head node in this order: Licensing Container Web VM Slurm VM Login Container NFS-gw VM Identity Container Shutdown head node Shutdown NAS1 Shutdown astro-th NAS Shutdown cee-water NAS Shutdown VAST Deactivate the cluster. This can be done by clicking on the cluster in the VMS UI maintenance page and hitting pause/deactivate or using VCLI (cluster deactivate) to do the same. Wait until the entire cluster deactivates cleanly on its own. Run these on any cnode to initiate shutdown. Cnodes shut down first, followed by Dnodes. #verify clush is healthy clush -a hostname #shutdown the cluster clush -g cnodes 'sudo /usr/sbin/shutdown -h +2'; clush -g dnodes 'sudo /usr/sbin/shutdown -h +5' Physically unplug power - the nodes come up automatically with power restoration Power Up Power up VAST Plug in in the D-nodes and wait for them to power up. Wait 10 minutes to ensure the D-nodes start cleanly. Plug in the C-nodes and wait for them to power up. Wait 10 minutes to ensure the C-nodes start cleanly. From the web UI, activate the cluster. Power up cee-water NAS Power up astro-th NAS in person, IPMI not yet set up Poewr up NAS1 Power up head node Power up head node VMs/containers Identity Container NFS-gw VM Login Container Slurm VM Web VM Licensing Container Power up all compute nodes Allow access from Palo Alto Perirhinal Power Down Pause all active VMs Shutdown from Web UI Power Up Power up in-person, IPMI is not available CloudLab / OCT Power Down Power down head node Power Up Power up head node General After the above, shut down the PA-3220","title":"MGHPCC Shutdown"},{"location":"shutdown/shutdown/#procedures-for-mghpcc-shutdown","text":"","title":"Procedures for MGHPCC Shutdown"},{"location":"shutdown/shutdown/#unity-cluster","text":"","title":"Unity Cluster"},{"location":"shutdown/shutdown/#power-down","text":"Block access from the Palo firewall rule. Shutdown all Compute Nodes Shutdown the containers/VMs running on the head node in this order: Licensing Container Web VM Slurm VM Login Container NFS-gw VM Identity Container Shutdown head node Shutdown NAS1 Shutdown astro-th NAS Shutdown cee-water NAS Shutdown VAST Deactivate the cluster. This can be done by clicking on the cluster in the VMS UI maintenance page and hitting pause/deactivate or using VCLI (cluster deactivate) to do the same. Wait until the entire cluster deactivates cleanly on its own. Run these on any cnode to initiate shutdown. Cnodes shut down first, followed by Dnodes. #verify clush is healthy clush -a hostname #shutdown the cluster clush -g cnodes 'sudo /usr/sbin/shutdown -h +2'; clush -g dnodes 'sudo /usr/sbin/shutdown -h +5' Physically unplug power - the nodes come up automatically with power restoration","title":"Power Down"},{"location":"shutdown/shutdown/#power-up","text":"Power up VAST Plug in in the D-nodes and wait for them to power up. Wait 10 minutes to ensure the D-nodes start cleanly. Plug in the C-nodes and wait for them to power up. Wait 10 minutes to ensure the C-nodes start cleanly. From the web UI, activate the cluster. Power up cee-water NAS Power up astro-th NAS in person, IPMI not yet set up Poewr up NAS1 Power up head node Power up head node VMs/containers Identity Container NFS-gw VM Login Container Slurm VM Web VM Licensing Container Power up all compute nodes Allow access from Palo Alto","title":"Power Up"},{"location":"shutdown/shutdown/#perirhinal","text":"","title":"Perirhinal"},{"location":"shutdown/shutdown/#power-down_1","text":"Pause all active VMs Shutdown from Web UI","title":"Power Down"},{"location":"shutdown/shutdown/#power-up_1","text":"Power up in-person, IPMI is not available","title":"Power Up"},{"location":"shutdown/shutdown/#cloudlab-oct","text":"","title":"CloudLab / OCT"},{"location":"shutdown/shutdown/#power-down_2","text":"Power down head node","title":"Power Down"},{"location":"shutdown/shutdown/#power-up_2","text":"Power up head node","title":"Power Up"},{"location":"shutdown/shutdown/#general","text":"After the above, shut down the PA-3220","title":"General"},{"location":"slurm/concepts/","text":"Job Scheduling An HPC cluster's most important piece of software is the job scheduler. In very simple terms, consider this example: your personal computer has many jobs to run, such as opening an app, or playing a video. These jobs are automatically scheduled through your multi-core system. Your system decides when to run that job, on which cores, etc. An HPC job scheduler does the same thing, but across many nodes in a cluster. One key difference is that job schedulers are generally manual. The end-user will decide how many cores/ram their job needs to run, then the job scheduler decides where on the cluster to run it. Unity uses the Slurm job scheduler.","title":"Concepts"},{"location":"slurm/concepts/#job-scheduling","text":"An HPC cluster's most important piece of software is the job scheduler. In very simple terms, consider this example: your personal computer has many jobs to run, such as opening an app, or playing a video. These jobs are automatically scheduled through your multi-core system. Your system decides when to run that job, on which cores, etc. An HPC job scheduler does the same thing, but across many nodes in a cluster. One key difference is that job schedulers are generally manual. The end-user will decide how many cores/ram their job needs to run, then the job scheduler decides where on the cluster to run it. Unity uses the Slurm job scheduler.","title":"Job Scheduling"}]}