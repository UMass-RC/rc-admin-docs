{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"identity/","text":"Centralized Identity Management Every HPC cluster needs a centralized form of user management. This is because users will be using a number of nodes across the cluster, and all UIDs/GIDs need to be kept consistent on every system. Slurm in particular cares a lot about this. Unity uses LDAP to accomplish this. There are other alternatives, but LDAP seems to be the easiest and most mature solution.","title":"Introduction"},{"location":"identity/#centralized-identity-management","text":"Every HPC cluster needs a centralized form of user management. This is because users will be using a number of nodes across the cluster, and all UIDs/GIDs need to be kept consistent on every system. Slurm in particular cares a lot about this. Unity uses LDAP to accomplish this. There are other alternatives, but LDAP seems to be the easiest and most mature solution.","title":"Centralized Identity Management"},{"location":"identity/client/","text":"Our objective is to get our clients (all the nodes on the cluster), to talk to a centralized LDAP server to obtain all user information, so that we don't have to create local users on each node. To accomplish this, we will use NSLCD, which will contact the LDAP server, and NSCD, which will cache that information. Installing NSLCD/NSCD Install from Ubuntu package repositories: apt install libpam-ldapd During the installation, you will be asked several questions: LDAP URI is the location of the ldap server. In our case, thats ldap://identity/ DN (distinguished name) is the base DN, which is usually the domain component of the LDAP server. For Unity, dc=unity,dc=rc,dc=umass,dc=edu Now it will ask where to use LDAP info. You should check passwd , group , and shadow . At this point the system will be able to begin using the centralized users. However, the ldapsearch command won't work, because its config file is seperate. In /etc/ldap/ldap.conf : BASE dc=unity,dc=rc,dc=umass,dc=edu URI ldap://identity/","title":"OpenLDAP Client"},{"location":"identity/client/#installing-nslcdnscd","text":"Install from Ubuntu package repositories: apt install libpam-ldapd During the installation, you will be asked several questions: LDAP URI is the location of the ldap server. In our case, thats ldap://identity/ DN (distinguished name) is the base DN, which is usually the domain component of the LDAP server. For Unity, dc=unity,dc=rc,dc=umass,dc=edu Now it will ask where to use LDAP info. You should check passwd , group , and shadow . At this point the system will be able to begin using the centralized users. However, the ldapsearch command won't work, because its config file is seperate. In /etc/ldap/ldap.conf : BASE dc=unity,dc=rc,dc=umass,dc=edu URI ldap://identity/","title":"Installing NSLCD/NSCD"},{"location":"identity/server/","text":"Installing SLAPD Install slapd from the Ubuntu package repository apt install slapd During the installation, you will be asked several questions: Specify the admin password for the database Specify DIT, which is your domain organized into \"domain components\", or \"dc\": dc=unity,dc=rc,dc=umass,dc=edu would correspond to unity.rc.umass.edu . This is done by convention, although you can set this to anything arbitrary. Organization name is Users . Use MDB for database. Select yes for \"move old database\" Configuring SLAPD For Unity, we need to activate a number of schemas , which are files that you can important into the ldap database that add attributes / classes for ldap objects. The included schemas are located in /etc/ldap/schema . To activate a schema, use: ldapadd -Y EXTERNAL -H ldapi:/// -f <LOCATION OF SCHEMA> On Unity, we activate: cosine.ldif nis.ldif inetorgperson.ldif ssh.ldif this ldif is not included, but ldif files are plain text so you can create it: dn: cn=openssh-lpk,cn=schema,cn=config objectClass: olcSchemaConfig cn: openssh-lpk olcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' DESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) olcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY DESC 'MANDATORY: OpenSSH LPK objectclass' MAY ( sshPublicKey $ uid ) )","title":"OpenLDAP Server"},{"location":"identity/server/#installing-slapd","text":"Install slapd from the Ubuntu package repository apt install slapd During the installation, you will be asked several questions: Specify the admin password for the database Specify DIT, which is your domain organized into \"domain components\", or \"dc\": dc=unity,dc=rc,dc=umass,dc=edu would correspond to unity.rc.umass.edu . This is done by convention, although you can set this to anything arbitrary. Organization name is Users . Use MDB for database. Select yes for \"move old database\"","title":"Installing SLAPD"},{"location":"identity/server/#configuring-slapd","text":"For Unity, we need to activate a number of schemas , which are files that you can important into the ldap database that add attributes / classes for ldap objects. The included schemas are located in /etc/ldap/schema . To activate a schema, use: ldapadd -Y EXTERNAL -H ldapi:/// -f <LOCATION OF SCHEMA> On Unity, we activate: cosine.ldif nis.ldif inetorgperson.ldif ssh.ldif this ldif is not included, but ldif files are plain text so you can create it: dn: cn=openssh-lpk,cn=schema,cn=config objectClass: olcSchemaConfig cn: openssh-lpk olcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' DESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) olcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY DESC 'MANDATORY: OpenSSH LPK objectclass' MAY ( sshPublicKey $ uid ) )","title":"Configuring SLAPD"},{"location":"shutdown/shutdown/","text":"MGHPCC Annual Shutdown Note Items in italics should be done in-person Unity Cluster Power Down Block access from Palo Alto Shutdown all Compute Nodes from MAAS Shutdown the containers/VMs running on the head node in this order: Licensing LXC Web VM Login LXC Slurm VM NFS-GW VM Shutdown all NAS serving devices (excluding VAST) Shutdown VAST Deactivate the cluster. This can be done by clicking on the cluster in the VMS UI maintenance page and hitting pause/deactivate or using VCLI (cluster deactivate) to do the same. Wait until the entire cluster deactivates cleanly on its own. Connect to any cnode via ssh (web UI ip, username: vastdata , pass: vastdata ), and run these commands: #verify clush is healthy clush -a hostname #shutdown the cluster clush -g cnodes 'sudo /usr/sbin/shutdown -h +2'; clush -g dnodes 'sudo /usr/sbin/shutdown -h +5' Disconnect power to the cnodes and dnodes, either in-person or through remote access to the PDU. Shutdown identity LXC on head node Shutdown head node Power Up Power up head node Power up identity container Power up VAST Plug in in the D-nodes and wait for them to power up. Wait 10 minutes to ensure the D-nodes start cleanly. Plug in the C-nodes and wait for them to power up. Wait 10 minutes to ensure the C-nodes start cleanly. From the web UI, activate the cluster. Power up head node Power up all NAS devices Power up head node VMs/containers in this order: NFS-gw VM Login Container Slurm VM Web VM Licensing Container Power up all compute nodes from MAAS Allow access from Palo Alto Perirhinal Power Down Pause all active VMs Shutdown from Web UI Power Up Power up in-person, IPMI is not available CloudLab / OCT Power Down Power down head node Power Up Power up head node Pikes Power Down From CMGUI, power down all compute nodes From CMGUI, power down head node, you will lose connection to Bright Power Up Power up head node in-person Power up compute nodes from CMGUI","title":"MGHPCC Shutdown"},{"location":"shutdown/shutdown/#mghpcc-annual-shutdown","text":"Note Items in italics should be done in-person","title":"MGHPCC Annual Shutdown"},{"location":"shutdown/shutdown/#unity-cluster","text":"","title":"Unity Cluster"},{"location":"shutdown/shutdown/#power-down","text":"Block access from Palo Alto Shutdown all Compute Nodes from MAAS Shutdown the containers/VMs running on the head node in this order: Licensing LXC Web VM Login LXC Slurm VM NFS-GW VM Shutdown all NAS serving devices (excluding VAST) Shutdown VAST Deactivate the cluster. This can be done by clicking on the cluster in the VMS UI maintenance page and hitting pause/deactivate or using VCLI (cluster deactivate) to do the same. Wait until the entire cluster deactivates cleanly on its own. Connect to any cnode via ssh (web UI ip, username: vastdata , pass: vastdata ), and run these commands: #verify clush is healthy clush -a hostname #shutdown the cluster clush -g cnodes 'sudo /usr/sbin/shutdown -h +2'; clush -g dnodes 'sudo /usr/sbin/shutdown -h +5' Disconnect power to the cnodes and dnodes, either in-person or through remote access to the PDU. Shutdown identity LXC on head node Shutdown head node","title":"Power Down"},{"location":"shutdown/shutdown/#power-up","text":"Power up head node Power up identity container Power up VAST Plug in in the D-nodes and wait for them to power up. Wait 10 minutes to ensure the D-nodes start cleanly. Plug in the C-nodes and wait for them to power up. Wait 10 minutes to ensure the C-nodes start cleanly. From the web UI, activate the cluster. Power up head node Power up all NAS devices Power up head node VMs/containers in this order: NFS-gw VM Login Container Slurm VM Web VM Licensing Container Power up all compute nodes from MAAS Allow access from Palo Alto","title":"Power Up"},{"location":"shutdown/shutdown/#perirhinal","text":"","title":"Perirhinal"},{"location":"shutdown/shutdown/#power-down_1","text":"Pause all active VMs Shutdown from Web UI","title":"Power Down"},{"location":"shutdown/shutdown/#power-up_1","text":"Power up in-person, IPMI is not available","title":"Power Up"},{"location":"shutdown/shutdown/#cloudlab-oct","text":"","title":"CloudLab / OCT"},{"location":"shutdown/shutdown/#power-down_2","text":"Power down head node","title":"Power Down"},{"location":"shutdown/shutdown/#power-up_2","text":"Power up head node","title":"Power Up"},{"location":"shutdown/shutdown/#pikes","text":"","title":"Pikes"},{"location":"shutdown/shutdown/#power-down_3","text":"From CMGUI, power down all compute nodes From CMGUI, power down head node, you will lose connection to Bright","title":"Power Down"},{"location":"shutdown/shutdown/#power-up_3","text":"Power up head node in-person Power up compute nodes from CMGUI","title":"Power Up"},{"location":"slurm/concepts/","text":"Job Scheduling An HPC cluster's most important piece of software is the job scheduler. In very simple terms, consider this example: your personal computer has many jobs to run, such as opening an app, or playing a video. These jobs are automatically scheduled through your multi-core system. Your system decides when to run that job, on which cores, etc. An HPC job scheduler does the same thing, but across many nodes in a cluster. One key difference is that job schedulers are generally manual. The end-user will decide how many cores/ram their job needs to run, then the job scheduler decides where on the cluster to run it. Unity uses the Slurm job scheduler.","title":"Concepts"},{"location":"slurm/concepts/#job-scheduling","text":"An HPC cluster's most important piece of software is the job scheduler. In very simple terms, consider this example: your personal computer has many jobs to run, such as opening an app, or playing a video. These jobs are automatically scheduled through your multi-core system. Your system decides when to run that job, on which cores, etc. An HPC job scheduler does the same thing, but across many nodes in a cluster. One key difference is that job schedulers are generally manual. The end-user will decide how many cores/ram their job needs to run, then the job scheduler decides where on the cluster to run it. Unity uses the Slurm job scheduler.","title":"Job Scheduling"}]}